{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data import COCOInstance\n",
    "\n",
    "# typically we use train2017 (i.e. train2014 + minival35k) split as training data\n",
    "# COCO dataset actually has images without any objects annotated,\n",
    "# which must be skipped during training to prevent empty labels\n",
    "train_dataset = COCOInstance(splits='instances_train2017', skip_empty=True)\n",
    "# and val2014 (i.e. minival5k) test as validation data\n",
    "val_dataset = COCOInstance(splits='instances_val2017', skip_empty=False)\n",
    "\n",
    "print('Training images:', len(train_dataset))\n",
    "print('Validation images:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loading annotations into memory...\n",
    "Done (t=13.42s)\n",
    "creating index...\n",
    "index created!\n",
    "loading annotations into memory...\n",
    "Done (t=0.38s)\n",
    "creating index...\n",
    "index created!\n",
    "Training images: 117266\n",
    "Validation images: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_label, train_segm = train_dataset[6]\n",
    "bboxes = train_label[:, :4]\n",
    "cids = train_label[:, 4:5]\n",
    "print('image:', train_image.shape)\n",
    "print('bboxes:', bboxes.shape, 'class ids:', cids.shape)\n",
    "# segm is a list of polygons which are arrays of points on the object boundary\n",
    "print('masks', [[poly.shape for poly in polys] for polys in train_segm])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image: (500, 381, 3)\n",
    "bboxes: (9, 4) class ids: (9, 1)\n",
    "masks [[(95, 2)], [(32, 2)], [(31, 2)], [(50, 2)], [(54, 2)], [(13, 2)], [(24, 2)], [(10, 2), (15, 2)], [(21, 2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from gluoncv.utils import viz\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax = viz.plot_bbox(train_image, bboxes, labels=cids, class_names=train_dataset.classes, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\n",
    "from gluoncv import utils\n",
    "from mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short, max_size = 600, 1000  \n",
    "train_transform = presets.rcnn.MaskRCNNDefaultTrainTransform(short, max_size)\n",
    "val_transform = presets.rcnn.MaskRCNNDefaultValTransform(short, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.random.seed(233)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image2, train_label2, train_masks2 = train_transform(train_image, train_label, train_segm)\n",
    "print('tensor shape:', train_image2.shape)\n",
    "print('box and id shape:', train_label2.shape)\n",
    "print('mask shape', train_masks2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor shape: (3, 787, 600)\n",
    "box and id shape: (9, 5)\n",
    "mask shape (9, 787, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array(\n",
    "    (0.485, 0.456, 0.406))\n",
    "plt_image2 = (plt_image2 * 255).asnumpy().astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width, height = plt_image2.shape[1], plt_image2.shape[0]\n",
    "plt_image2 = viz.plot_mask(plt_image2, train_masks2)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax = viz.plot_bbox(plt_image2, train_label2[:, :4],\n",
    "                   labels=train_label2[:, 4:5],\n",
    "                   class_names=train_dataset.classes,\n",
    "                   ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Append, MaskRCNNTrainBatchify\n",
    "from mxnet.gluon.data import DataLoader\n",
    "\n",
    "batch_size = 2  \n",
    "num_workers = 0  \n",
    "\n",
    "train_bfn = Tuple(*[Append() for _ in range(3)])\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=train_bfn, last_batch='rollover', num_workers=num_workers)\n",
    "val_bfn = Tuple(*[Append() for _ in range(2)])\n",
    "val_loader = DataLoader(val_dataset.transform(val_transform), batch_size, shuffle=False,\n",
    "                        batchify_fn=val_bfn, last_batch='keep', num_workers=num_workers)\n",
    "\n",
    "for ib, batch in enumerate(train_loader):\n",
    "    if ib > 3:\n",
    "        break\n",
    "    print('data 0:', batch[0][0].shape, 'label 0:', batch[1][0].shape, 'mask 0:', batch[2][0].shape)\n",
    "    print('data 1:', batch[0][1].shape, 'label 1:', batch[1][1].shape, 'mask 1:', batch[2][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data 0: (1, 3, 600, 901) label 0: (1, 2, 5) mask 0: (1, 2, 600, 901)\n",
    "data 1: (1, 3, 800, 600) label 1: (1, 1, 5) mask 1: (1, 1, 800, 600)\n",
    "data 0: (1, 3, 798, 600) label 0: (1, 2, 5) mask 0: (1, 2, 798, 600)\n",
    "data 1: (1, 3, 600, 600) label 1: (1, 18, 5) mask 1: (1, 18, 600, 600)\n",
    "data 0: (1, 3, 600, 800) label 0: (1, 1, 5) mask 0: (1, 1, 600, 800)\n",
    "data 1: (1, 3, 600, 600) label 1: (1, 5, 5) mask 1: (1, 5, 600, 600)\n",
    "data 0: (1, 3, 600, 800) label 0: (1, 2, 5) mask 0: (1, 2, 600, 800)\n",
    "data 1: (1, 3, 800, 600) label 1: (1, 21, 5) mask 1: (1, 21, 800, 600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluoncv import model_zoo\n",
    "\n",
    "net = model_zoo.get_model('mask_rcnn_resnet50_v1b_coco', pretrained_base=False)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaskRCNN(\n",
    "  (features): HybridSequential(\n",
    "    (0): Conv2D(None -> 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "    (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "    (2): Activation(relu)\n",
    "    (3): MaxPool2D(size=(3, 3), stride=(2, 2), padding=(1, 1), ceil_mode=False, global_pool=False, pool_type=max, layout=NCHW)\n",
    "    (4): HybridSequential(\n",
    "      (0): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu3): Activation(relu)\n",
    "        (downsample): HybridSequential(\n",
    "          (0): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        )\n",
    "      )\n",
    "      (1): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (2): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=64)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "    )\n",
    "    (5): HybridSequential(\n",
    "      (0): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu3): Activation(relu)\n",
    "        (downsample): HybridSequential(\n",
    "          (0): Conv2D(None -> 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        )\n",
    "      )\n",
    "      (1): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (2): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (3): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=128)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "    )\n",
    "    (6): HybridSequential(\n",
    "      (0): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        (relu3): Activation(relu)\n",
    "        (downsample): HybridSequential(\n",
    "          (0): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        )\n",
    "      )\n",
    "      (1): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (2): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (3): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (4): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (5): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=256)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=1024)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (top_features): HybridSequential(\n",
    "    (0): HybridSequential(\n",
    "      (0): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
    "        (relu3): Activation(relu)\n",
    "        (downsample): HybridSequential(\n",
    "          (0): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "          (1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
    "        )\n",
    "      )\n",
    "      (1): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "      (2): BottleneckV1b(\n",
    "        (conv1): Conv2D(None -> 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn1): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu1): Activation(relu)\n",
    "        (conv2): Conv2D(None -> 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "        (bn2): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=512)\n",
    "        (relu2): Activation(relu)\n",
    "        (conv3): Conv2D(None -> 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
    "        (bn3): BatchNorm(axis=1, eps=1e-05, momentum=0.9, fix_gamma=False, use_global_stats=True, in_channels=2048)\n",
    "        (relu3): Activation(relu)\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (class_predictor): Dense(None -> 81, linear)\n",
    "  (box_predictor): Dense(None -> 320, linear)\n",
    "  (cls_decoder): MultiPerClassDecoder(\n",
    "\n",
    "  )\n",
    "  (box_decoder): NormalizedBoxCenterDecoder(\n",
    "    (corner_to_center): BBoxCornerToCenter(\n",
    "\n",
    "    )\n",
    "  )\n",
    "  (rpn): RPN(\n",
    "    (anchor_generator): RPNAnchorGenerator(\n",
    "\n",
    "    )\n",
    "    (conv1): HybridSequential(\n",
    "      (0): Conv2D(None -> 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "      (1): Activation(relu)\n",
    "    )\n",
    "    (score): Conv2D(None -> 15, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (loc): Conv2D(None -> 60, kernel_size=(1, 1), stride=(1, 1))\n",
    "    (region_proposer): RPNProposal(\n",
    "      (_box_to_center): BBoxCornerToCenter(\n",
    "\n",
    "      )\n",
    "      (_box_decoder): NormalizedBoxCenterDecoder(\n",
    "        (corner_to_center): BBoxCornerToCenter(\n",
    "\n",
    "        )\n",
    "      )\n",
    "      (_clipper): BBoxClipToImage(\n",
    "\n",
    "      )\n",
    "    )\n",
    "  )\n",
    "  (sampler): RCNNTargetSampler(\n",
    "\n",
    "  )\n",
    "  (mask): Mask(\n",
    "    (deconv): Conv2DTranspose(256 -> 0, kernel_size=(2, 2), stride=(2, 2))\n",
    "    (mask): Conv2D(None -> 80, kernel_size=(1, 1), stride=(1, 1))\n",
    "  )\n",
    "  (mask_target): MaskTargetGenerator(\n",
    "\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "\n",
    "x = mx.nd.zeros(shape=(1, 3, 600, 800))\n",
    "net.initialize()\n",
    "cids, scores, bboxes, masks = net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import autograd\n",
    "\n",
    "with autograd.train_mode():\n",
    "    gt_box = mx.nd.zeros(shape=(1, 1, 4))\n",
    "    gt_label = mx.nd.zeros(shape=(1, 1, 1))\n",
    "    cls_pred, box_pred, mask_pred, roi, samples, matches, rpn_score, rpn_box, anchors, \\\n",
    "    cls_targets, box_targets, box_masks, indices = net(x, gt_box, gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rpn_cls_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n",
    "\n",
    "rpn_box_loss = mx.gluon.loss.HuberLoss(rho=1 / 9.)  \n",
    "\n",
    "rcnn_cls_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "rcnn_box_loss = mx.gluon.loss.HuberLoss()  # == smoothl1\n",
    "\n",
    "rcnn_mask_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = presets.rcnn.MaskRCNNDefaultTrainTransform(short, max_size, net)\n",
    "batchify_fn = MaskRCNNTrainBatchify(net)\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n",
    "                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(train_loader):\n",
    "    if ib > 0:\n",
    "        break\n",
    "    with autograd.train_mode():\n",
    "        for data, label, masks, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n",
    "            label = label.expand_dims(0)\n",
    "            gt_label = label[:, :, 4:5]\n",
    "            gt_box = label[:, :, :4]\n",
    "            # network forward\n",
    "            cls_pred, box_pred, mask_pred, roi, samples, matches, rpn_score, rpn_box, anchors, \\\n",
    "            cls_targets, box_targets, box_masks, indices = \\\n",
    "                net(data.expand_dims(0), gt_box, gt_label)\n",
    "\n",
    "            # generate targets for mask head\n",
    "            roi = mx.nd.concat(\n",
    "                *[mx.nd.take(roi[i], indices[i]) for i in range(indices.shape[0])], dim=0) \\\n",
    "                .reshape((indices.shape[0], -1, 4))\n",
    "            m_cls_targets = mx.nd.concat(\n",
    "                *[mx.nd.take(cls_targets[i], indices[i]) for i in range(indices.shape[0])], dim=0) \\\n",
    "                .reshape((indices.shape[0], -1))\n",
    "            matches = mx.nd.concat(\n",
    "                *[mx.nd.take(matches[i], indices[i]) for i in range(indices.shape[0])], dim=0) \\\n",
    "                .reshape((indices.shape[0], -1))\n",
    "            mask_targets, mask_masks = net.mask_target(roi, masks.expand_dims(0), matches,\n",
    "                                                       m_cls_targets)\n",
    "\n",
    "            print('data:', data.shape)\n",
    "            # box and class labels\n",
    "            print('box:', gt_box.shape)\n",
    "            print('label:', gt_label.shape)\n",
    "            # -1 marks ignored label\n",
    "            print('rpn cls label:', rpn_cls_targets.shape)\n",
    "            # mask out ignored box label\n",
    "            print('rpn box label:', rpn_box_targets.shape)\n",
    "            print('rpn box mask:', rpn_box_masks.shape)\n",
    "            # rcnn does not have ignored label\n",
    "            print('rcnn cls label:', cls_targets.shape)\n",
    "            # mask out ignored box label\n",
    "            print('rcnn box label:', box_targets.shape)\n",
    "            print('rcnn box mask:', box_masks.shape)\n",
    "            print('rcnn mask label:', mask_targets.shape)\n",
    "            print('rcnn mask mask:', mask_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data: (3, 831, 600)\n",
    "box: (1, 1, 4)\n",
    "label: (1, 1, 1)\n",
    "rpn cls label: (1, 29640)\n",
    "rpn box label: (1, 29640, 4)\n",
    "rpn box mask: (1, 29640, 4)\n",
    "rcnn cls label: (1, 128)\n",
    "rcnn box label: (1, 32, 80, 4)\n",
    "rcnn box mask: (1, 32, 80, 4)\n",
    "rcnn mask label: (1, 32, 80, 14, 14)\n",
    "rcnn mask mask: (1, 32, 80, 14, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(train_loader):\n",
    "    if ib > 0:\n",
    "        break\n",
    "    with autograd.record():\n",
    "        for data, label, masks, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n",
    "            label = label.expand_dims(0)\n",
    "            gt_label = label[:, :, 4:5]\n",
    "            gt_box = label[:, :, :4]\n",
    "            # network forward\n",
    "            cls_preds, box_preds, mask_preds, roi, samples, matches, rpn_score, rpn_box, anchors, \\\n",
    "                cls_targets, box_targets, box_masks, indices = \\\n",
    "                net(data.expand_dims(0), gt_box, gt_label)\n",
    "\n",
    "            # generate targets for mask head\n",
    "            roi = mx.nd.concat(\n",
    "                *[mx.nd.take(roi[i], indices[i]) for i in range(indices.shape[0])], dim=0) \\\n",
    "                .reshape((indices.shape[0], -1, 4))\n",
    "            m_cls_targets = mx.nd.concat(\n",
    "                *[mx.nd.take(cls_targets[i], indices[i]) for i in range(indices.shape[0])], dim=0) \\\n",
    "                .reshape((indices.shape[0], -1))\n",
    "            matches = mx.nd.concat(\n",
    "                *[mx.nd.take(matches[i], indices[i]) for i in range(indices.shape[0])], dim=0) \\\n",
    "                .reshape((indices.shape[0], -1))\n",
    "            mask_targets, mask_masks = net.mask_target(roi, masks.expand_dims(0), matches,\n",
    "                                                       m_cls_targets)\n",
    "\n",
    "            # losses of rpn\n",
    "            rpn_score = rpn_score.squeeze(axis=-1)\n",
    "            num_rpn_pos = (rpn_cls_targets >= 0).sum()\n",
    "            rpn_loss1 = rpn_cls_loss(rpn_score, rpn_cls_targets,\n",
    "                                     rpn_cls_targets >= 0) * rpn_cls_targets.size / num_rpn_pos\n",
    "            rpn_loss2 = rpn_box_loss(rpn_box, rpn_box_targets,\n",
    "                                     rpn_box_masks) * rpn_box.size / num_rpn_pos\n",
    "\n",
    "            # losses of rcnn\n",
    "            num_rcnn_pos = (cls_targets >= 0).sum()\n",
    "            rcnn_loss1 = rcnn_cls_loss(cls_preds, cls_targets,\n",
    "                                       cls_targets >= 0) * cls_targets.size / cls_targets.shape[\n",
    "                             0] / num_rcnn_pos\n",
    "            rcnn_loss2 = rcnn_box_loss(box_preds, box_targets, box_masks) * box_preds.size / \\\n",
    "                         box_preds.shape[0] / num_rcnn_pos\n",
    "\n",
    "            # loss of mask\n",
    "            mask_loss = rcnn_mask_loss(mask_preds, mask_targets, mask_masks) * mask_targets.size / \\\n",
    "                        mask_targets.shape[0] / mask_masks.sum()\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Learning_py3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
